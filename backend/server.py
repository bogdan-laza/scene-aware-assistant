import os
import torch
import io
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from typing import Optional, List
from PIL import Image
from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig, AutoConfig
import uvicorn
from huggingface_hub import login
import time

# --- Configuration ---
# The ID of your fine-tuned model (weights)
MY_MODEL_ID = "calinMoglan/pedestrian-detector-v1"
# The ID of the base architecture (used for configuration and processor files)
BASE_ARCH_ID = "LiquidAI/LFM2-VL-1.6B"

# System prompts to guide the model's behavior
SAFETY_SYSTEM_PROMPT = (
    "You are a precise safety assistant for a blind pedestrian. "
    "Output ONLY the requested classification or warning. "
    "Do NOT repeat the user prompt. Be concise and urgent."
)

GENERAL_SYSTEM_PROMPT = (
    "You are a helpful AI assistant describing images for a visually impaired user. "
    "Be helpful, accurate, and concise."
)

# Global variables to hold the model in memory
model = None
processor = None
device = None

ALLOWED_IMAGE_TYPES = {"image/jpeg", "image/png", "image/webp"}
MAX_IMAGE_BYTES = 10 * 1024 * 1024

@asynccontextmanager
async def lifespan(app: FastAPI):
    global model, processor, device
    
    # Authenticate with Hugging Face if a token is present
    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        login(token=hf_token)
    else:
        print("Warning: No HF_TOKEN found. Make sure you have access to the models.")

    # Determine if we are running on GPU (CUDA) or CPU
    use_cuda = torch.cuda.is_available()
    device = "cuda" if use_cuda else "cpu"
    print(f"Server running on: {device}")

    try:
        # Configure 4-bit quantization to reduce memory usage on the GPU
        bnb_config = None
        if use_cuda:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )

        print(f"Loading Configuration from: {BASE_ARCH_ID} ...")
        # Load the architecture configuration from the original LiquidAI repository
        # This ensures we have the correct Python code definitions for the model structure
        config = AutoConfig.from_pretrained(BASE_ARCH_ID, trust_remote_code=True)
        
        print(f"Loading Weights from: {MY_MODEL_ID} ...")
        # Load the actual fine-tuned weights from your repository
        model = AutoModelForImageTextToText.from_pretrained(
            MY_MODEL_ID,
            config=config,
            quantization_config=bnb_config if use_cuda else None,
            device_map="auto" if use_cuda else "cpu",
            dtype=torch.float16 if use_cuda else torch.bfloat16,
            trust_remote_code=True
        )
        
        print(f"Loading processor from: {BASE_ARCH_ID} ...")
        # Load the image processor (handles resizing and normalization)
        processor = AutoProcessor.from_pretrained(
            BASE_ARCH_ID, 
            trust_remote_code=True,
            max_image_tokens=961
        )
        print("Model loaded successfully.")

    except Exception as e:
        print(f"Error loading models: {e}")
        raise e
    
    yield
    print("Server shutting down.")

app = FastAPI(title="Scene Assistant Backend", lifespan=lifespan)
"""
def run_inference_sync(image: Image.Image, prompt_text: str, system_prompt: str) -> str:
    
   # Helper function to run the model inference synchronously.
    #It prepares the inputs, generates the text, and decodes the output.
    
    global processor, model
    
    conversation = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt_text},
            ],
        },
    ]
    
    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(images=[image], text=text_prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=15, 
            do_sample=False,
            repetition_penalty=1.0 
        )

    # Decode only the new tokens generated by the model
    generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    return generated_text.strip()
    """

def run_inference_sync(image: Image.Image, prompt_text: str, system_prompt: str) -> str:
    global processor, model
    
    print("--- STARTING INFERENCE ---")
    start_time = time.time()
    
    conversation = [
        {"role": "system", "content": [{"type": "text", "text": system_prompt}]},
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt_text},
            ],
        },
    ]
    
    print(f"1. Processing Inputs... ({time.time() - start_time:.2f}s)")
    text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(images=[image], text=text_prompt, return_tensors="pt").to(model.device)

    print(f"2. Generating Answer... ({time.time() - start_time:.2f}s)")
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=15, 
            do_sample=False,
            repetition_penalty=1.0 
        )

    print(f"3. Decoding... ({time.time() - start_time:.2f}s)")
    generated_ids = output_ids[:, inputs['input_ids'].shape[1]:]
    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    
    total_time = time.time() - start_time
    print(f"--- FINISHED in {total_time:.2f} SECONDS ---")
    
    return generated_text.strip()

def clean_model_response(raw_text: str, valid_phrases: List[str], default_response: str) -> str:
    """
    Validates the model's output against a list of allowed phrases.
    If the model hallucinates or is too verbose, it falls back to a default.
    """
    raw_lower = raw_text.lower()
    for phrase in valid_phrases:
        if phrase.lower() in raw_lower:
            return phrase
    if len(raw_text) > 100:
        return default_response
    return raw_text

async def validate_image(file: Optional[UploadFile]) -> None:
    """
    Checks if the uploaded file is a valid image and within size limits.
    """
    if file is None: raise HTTPException(status_code=400, detail="Missing file")
    if file.content_type not in ALLOWED_IMAGE_TYPES: raise HTTPException(status_code=400, detail="File must be an image")
    data = await file.read()
    if not data: raise HTTPException(status_code=400, detail="Missing file")
    if len(data) > MAX_IMAGE_BYTES: raise HTTPException(status_code=400, detail="Image too large")
    await file.seek(0)

@app.get("/health")
def health_check():
    return {"status": "OK", "model": MY_MODEL_ID}

@app.post("/obstacles")
async def obstacles(file: Optional[UploadFile] = File(None)):
    """
    Endpoint for detecting immediate dangers (cars, obstacles, etc.).
    Uses a strict prompt to force the model into specific classification categories.
    """
    await validate_image(file)
    if model is None: raise HTTPException(status_code=503, detail="Model not loaded")
    
    contents = await file.read()
    image = Image.open(io.BytesIO(contents)).convert("RGB")
    
    prompt = (
        "Analyze the path ahead. Output ONLY one of the following sentences:\n"
        "- 'Caution: Car approaching'\n"
        "- 'Caution: Obstacle on path'\n"
        "- 'Clear: Path is safe'\n"
        "- 'Caution: Unpaved surface'"
    )
    
    try:
        raw_response = await asyncio.to_thread(
            run_inference_sync, image, prompt, SAFETY_SYSTEM_PROMPT
        )
        
        valid_options = ["Caution: Car approaching", "Caution: Obstacle on path", "Clear: Path is safe", "Caution: Unpaved surface"]
        clean_result = clean_model_response(raw_response, valid_options, "Caution: Unknown danger")
        
        return JSONResponse(content={"type": "obstacle_detection", "result": clean_result, "confidence": 0.65})
    except Exception as e:
        print(f"Error in obstacles: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/crosswalk")
async def crosswalk(file: Optional[UploadFile] = File(None)):
    """
    Endpoint for detecting pedestrian crosswalks.
    """
    await validate_image(file)
    if model is None: raise HTTPException(status_code=503, detail="Model not loaded")
    
    contents = await file.read()
    image = Image.open(io.BytesIO(contents)).convert("RGB")

    prompt = "Check the path ahead for a pedestrian crosswalk."
    system_prompt = (
        "You are an advanced visual assistant for pedestrian safety.\n"
        "Analyze the image and output ONLY one of the following classification labels:\n"
        "- \"Safe crosswalk detected\": if a pedestrian crosswalk (white stripes) is clearly visible on the road.\n"
        "- \"No crosswalk\": if no crosswalk is visible.\n"
        "Do not provide explanations. Output only the label."
    )
    
    try:
        raw_response = await asyncio.to_thread(
            run_inference_sync, image, prompt, system_prompt
        )

        # Log the raw response for monitoring
        print(f"Debug Crosswalk Model: '{raw_response}'")
        
        clean_response = raw_response.replace('"', '').strip()
        if "Safe crosswalk detected" in clean_response:
            clean_result = "Safe crosswalk detected"
        else:
            clean_result = "No crosswalk"
            
        return JSONResponse(content={
            "type": "crosswalk_analysis", 
            "result": clean_result, 
            "confidence": 0.90 
        })
    except Exception as e:
        print(f"Error in crosswalk: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/custom")
async def custom(file: Optional[UploadFile] = File(None), prompt: Optional[str] = Form(None)):
    """
    Endpoint for general user queries (e.g., 'What color is the shirt?').
    """
    await validate_image(file)
    if not prompt or not prompt.strip(): raise HTTPException(status_code=400, detail="Missing prompt")
    if model is None: raise HTTPException(status_code=503, detail="Model not loaded")
    
    contents = await file.read()
    image = Image.open(io.BytesIO(contents)).convert("RGB")
    
    try:
        response = await asyncio.to_thread(
            run_inference_sync, image, prompt.strip(), GENERAL_SYSTEM_PROMPT
        )
        
        return JSONResponse(content={"type": "custom_query", "prompt": prompt.strip(), "result": response, "confidence": 0.65})
    except Exception as e:
        print(f"Error in custom: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)